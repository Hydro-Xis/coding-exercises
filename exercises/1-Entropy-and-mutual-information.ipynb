{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820fb7e0",
   "metadata": {},
   "source": [
    "This notebook aims to provide a gentle introduction of Shannon's entropy and mutual information.\n",
    "\n",
    "**Please do not distribute this material without consent of the author.\n",
    "@Copyright 2025\n",
    "Peishi Jiang**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f50c89",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# What is information theory?\n",
    "Information theory is a mathematical method to quantify the nonlinear dependencies in a multivariate system. It originates from electronic engineering to study the storage, quantification, and communication of information. Claude Shannon pioneered this field and proposed the concept of Shannon's entropy in the 1940s. Since then, this mathematical branch has gone through great development and applications in various field, including but not limited to, electronic engineering, mathematics, computer science, phyics, neurobiology, and earth science/hydrology.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ab397",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "We use Shannon's entropy here, denoted of $H$. Given a variable $X$, $H$ quantifies the overall uncertainty (or variability) of $X$ as:\n",
    "\\begin{align}\n",
    " H(X) &= - \\sum_{x \\in X} p(x) \\log(p(x)) \\tag{1},\n",
    "\\end{align}\n",
    "where $p(x)$ is the discretized probability of the realization of $X=x$.\n",
    "\n",
    "For two variables $X$ and $Y$, their overall uncertainty can be calculated through the joint entropy as below:\n",
    "\\begin{align}\n",
    " H(X,Y) &= - \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log(p(x,y)) \\tag{2}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6f075e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Entropy's properties\n",
    "Shannon's entropy has the following nice properties (**Prove them!**):\n",
    "1. Nonnegativity: $H \\geq 0$. In other words, the entropy is always nonnegative.\n",
    "2. Chain rule: $H(X,Y) = H(X) + H(Y|X)$, where $H(Y|X)=-\\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log(p(y|x))$ is the conditional entropy. This property states that the joint uncertainty of $X$ and $Y$ is the sum of $X$'s uncertainty ($H(X)$) and the remaining uncertainty of $Y$ given the knowledge of $X$ ($H(Y|X)$).\n",
    "3. $H(X|Y) \\leq H(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b2f2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Maximum and minimum entropy (univariate case)\n",
    "Now, let's move onto the calculation of the entropy $H$ and see under what conditions it reaches maximum/minimum.\n",
    "\n",
    "OK, say a variable $X$ has two states $X=a$ and $X=b$, with probabilities $P(X=a)$ and $P(X=b)$, respectively, such that $P(X=a) + P(X=b) = 1$. Instead of mannually calculating it (which you can!), we are going to use Python programming to compute it.\n",
    "\n",
    "Let's first define a function to calculate the entropy --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "599bd82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def computeEntropy(pdf, dims=None, base=np.e):\n",
    "    \"\"\"\n",
    "    Compute the entropy along dims\n",
    "    pdf: probability mass distribution with shape (Nx, Ny, Nz, ...)\n",
    "    dims: the dimension indices\n",
    "    \"\"\"\n",
    "    shape = pdf.shape\n",
    "    \n",
    "    # Get the number of variables/dimensions\n",
    "    D = len(shape)\n",
    "\n",
    "    # Get the indices of all dimensions/variabiels\n",
    "    all_dims  = set(range(D))\n",
    "\n",
    "    # Compute the marginal PDF\n",
    "    if dims is None:\n",
    "        pdf_dim = pdf\n",
    "    elif all_dims == set(dims):\n",
    "        pdf_dim = pdf\n",
    "    else:\n",
    "        pdf_dim = pdf.sum(axis=tuple(all_dims-set(dims)))\n",
    "\n",
    "    # Compute the entropy\n",
    "    log_pdf_dim = np.ma.filled(np.log(np.ma.masked_equal(pdf_dim, 0)), 0)\n",
    "    return -np.sum(pdf*log_pdf_dim / np.log(base))\n",
    "\n",
    "# def computeEntropy(pdf, base=np.e):\n",
    "#     \"\"\"\n",
    "#     Compute the entropy based on pdf\n",
    "#     pdf: a numpy array of probability such that sum(pdf)=1\n",
    "#     base: the logarithm base\n",
    "#     \"\"\"\n",
    "#     log_pdf = np.ma.filled(np.log(np.ma.masked_equal(pdf, 0)), 0)\n",
    "#     return -np.sum(pdf*log_pdf / np.log(base))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013d7e1",
   "metadata": {},
   "source": [
    "Scenario 1: X is determinant such that $P(X=a)=1$ and $P(X=b)=0$. What is $H(X)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d7bcabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1: -0.0 nats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/q6wyxw8x1_s5rh1mqvfb8vsr0000gn/T/ipykernel_19027/1805999971.py:25: RuntimeWarning: divide by zero encountered in log\n",
      "  log_pdf_dim = np.ma.filled(np.log(np.ma.masked_equal(pdf_dim, 0)), 0)\n"
     ]
    }
   ],
   "source": [
    "P1 = np.array([1, 0])\n",
    "H1 = computeEntropy(P1)\n",
    "print(f\"H1: {H1} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e0af3",
   "metadata": {},
   "source": [
    "$H(X) = 0$ ! This means there is no variability or uncertainty associated with $X$, which totally makes sense since $X$ is deterministic. Let's consider two additional scenarios:\n",
    "\n",
    "Scenario 2: $P(X=a)=0.1$ and $P(X=b)=0.9$\n",
    "\n",
    "Scenario 3: $P(X=a)=0.5$ and $P(X=b)=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb82d849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2: 0.3250829733914482 nats\n",
      "H3: 0.6931471805599453 nats\n"
     ]
    }
   ],
   "source": [
    "P2, P3 = np.array([0.1, 0.9]), np.array([0.5, 0.5])\n",
    "H2 = computeEntropy(P2)\n",
    "H3 = computeEntropy(P3)\n",
    "print(f\"H2: {H2} nats\")\n",
    "print(f\"H3: {H3} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c720a",
   "metadata": {},
   "source": [
    "Why does $H2 < H3$? \n",
    "\n",
    "Intuitively, this is because scenario 2 is more deterministic than scenario 3 since there is a much higher chance that $X=b$. On the other hand, scenario 3 gives equal probability to $X=a$ and $X=b$.\n",
    "\n",
    "In fact, one can prove that the entropy is maximized when the distribution follows a uniform distribution! Note that scenario 3 is exactly a discretized uniform distribution with only two realizations. In a generalized discrete uniform distribution with $N$ realizations, $H = \\log(\\frac{1}{N})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e836400",
   "metadata": {},
   "source": [
    "## Maximum and minimum entropy (bivariate case)\n",
    "The same philosophy is applicable to a bivariate case.\n",
    "\n",
    "Now consider another variable $Y$ that takes on values $c$ and $d$. \n",
    "\n",
    "Scenario 4: The joint probabilty of $X$ and $Y$ are given by:\n",
    "\\begin{align}\n",
    " P(X=a,Y=c) &= 0.1 \\notag\\\\\n",
    " P(X=b,Y=c) &= 0.2 \\notag\\\\\n",
    " P(X=a,Y=d) &= 0.3 \\notag\\\\\n",
    " P(X=b,Y=d) &= 0.4. \\notag\n",
    "\\end{align}\n",
    "\n",
    "Their joint entropy is --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd6d3516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H4: 1.2798542258336676 nats\n"
     ]
    }
   ],
   "source": [
    "P4 = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "H4 = computeEntropy(P4)\n",
    "print(f\"H4: {H4} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd3057e",
   "metadata": {},
   "source": [
    "Scenario 5: What if it follows uniform distribution? We are expecting that the joint entropy would be larger than $H4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abd9f453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H5: 1.3862943611198906 nats\n"
     ]
    }
   ],
   "source": [
    "P5 = np.array([[0.25, 0.25], [0.25, 0.25]])\n",
    "H5 = computeEntropy(P5)\n",
    "print(f\"H5: {H5} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3fdf8",
   "metadata": {},
   "source": [
    "## What's the units of entropy?\n",
    "It depends on what base is used in the log! If it is a natural base (as what has been used in the previous cases), the units is \"nats\". If the base adopts 2, the units is \"bits\"!\n",
    "\n",
    "Now, let's revisit the three scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0841d6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1: -0.0 bits\n",
      "H2: 0.4689955935892812 bits\n",
      "H3: 1.0 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/q6wyxw8x1_s5rh1mqvfb8vsr0000gn/T/ipykernel_19027/1805999971.py:25: RuntimeWarning: divide by zero encountered in log\n",
      "  log_pdf_dim = np.ma.filled(np.log(np.ma.masked_equal(pdf_dim, 0)), 0)\n"
     ]
    }
   ],
   "source": [
    "base = 2.\n",
    "H1 = computeEntropy(P1, base=2)\n",
    "H2 = computeEntropy(P2, base=2)\n",
    "H3 = computeEntropy(P3, base=2)\n",
    "print(f\"H1: {H1} bits\")\n",
    "print(f\"H2: {H2} bits\")\n",
    "print(f\"H3: {H3} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d7e34",
   "metadata": {},
   "source": [
    "Now, the results are different. Sometimes, bits are more intuitive for binary cases. Here, $H_3=1$ since $\\log_2(1/2) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5696d0",
   "metadata": {},
   "source": [
    "## Differential entropy\n",
    "\n",
    "We have covered Shannon's entropy applied to the discretized probability space. What about continuous probability distribution? Well, this is where differential entropy is applied (denoted as lower case $h$):\n",
    "\\begin{align}\n",
    " h(X) &= - \\int_{x \\in X} f(x) \\log(f(x)) \\mathrm{d}x\\tag{3},\n",
    "\\end{align}\n",
    "where $f$ is the probability density function of $X$.\n",
    "\n",
    "Differential entropy is very useful when the functional form of $f$ is given and the analytical solutions of $h$ can be derived. Take the uniform distribution that are defined over $[a, b]$ as an example. The corresponding $h$ can be derived as:\n",
    "\\begin{align}\n",
    " h(X) = \\log(b-a)\\tag{4}.\n",
    "\\end{align}\n",
    "\n",
    "For Gaussian distribution $f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{\\frac{-x^2}{2\\sigma^2}}$, the corresponding $h$ is given as:\n",
    "\\begin{align}\n",
    " h(X) = \\frac{1}{2}\\log(2\\pi e \\sigma^2)\\tag{5}.\n",
    "\\end{align}\n",
    "\n",
    "**Note that differential entropy can be negative!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf370eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Mutual information\n",
    "\n",
    "Consider two random variables $X$ and $Y$ with a joint probability distribution $p(x,y)$. The mutual information $I(X;Y)$ quantifies the shared information or uncertainty between $X$ and $Y$, and it is given by:\n",
    "\\begin{align}\n",
    " I(X;Y) &= \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log(\\frac{p(x,y)}{p(x)p(y)})\\tag{6}.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b334a1",
   "metadata": {},
   "source": [
    "## Mutual information's properties\n",
    "\n",
    "While it might be a bit abstract at the first glance, Eq.(6) can in fact be converted into:\n",
    "\\begin{align}\n",
    " I(X;Y) &= H(Y) - H(Y|X)\\tag{7a} \\\\\n",
    "        &= H(X) - H(X|Y)\\tag{7b} \\\\\n",
    "        &= H(X) + H(Y) - H(X,Y)\\tag{7c}\n",
    "\\end{align}\n",
    "The above equations describes that $I(X;Y)$ is the difference between the entropy of one variable and the remaining uncertainty of this variable given the knowledge of the other other. As a result, $I(X;Y)$ is a symmetric metric, such that $I(X;Y)=I(Y;X)$.\n",
    "\n",
    "**Properties of $I(X;Y)$**:\n",
    "1. $I(X;Y) = H(Y) - H(Y|X) = H(X) - H(X|Y) = H(X) + H(Y) - H(X,Y)$.\n",
    "2. $I(X;Y) = 0$ when $X$ and $Y$ are independent such that $P(X,Y) = P(X)P(Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd111a5",
   "metadata": {},
   "source": [
    "Let's define a function that calculates $I(X;Y)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea6b5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMI(pdf, dims1, dims2, base=np.e):\n",
    "    \"\"\"\n",
    "    Compute the mutual information given two sets of dimensions\n",
    "    dims1 -- the first dimension indices\n",
    "    dims2 -- the second dimension indices\n",
    "    normalized -- whether to normalize the mutual information through dividing it by the output entropy\n",
    "    \"\"\"\n",
    "    # Compute the entropies\n",
    "    h12 = computeEntropy(pdf, dims1+dims2, base)\n",
    "    h1  = computeEntropy(pdf, dims1, base)\n",
    "    h2  = computeEntropy(pdf, dims2, base)\n",
    "\n",
    "    # Compute the mutual information\n",
    "    return h1 + h2 - h12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8139b",
   "metadata": {},
   "source": [
    "Let's revisit scenarios 4 and 5. What are the mutual information of $X$ and $Y$ for the two cases? Which is larger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83529072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1: 0.08875152926920249 nats\n",
      "I2: 0.0 nats\n"
     ]
    }
   ],
   "source": [
    "I1 = computeMI(P4, dims1=[0], dims2=[1])\n",
    "I2 = computeMI(P5, dims1=[0], dims2=[1])\n",
    "\n",
    "print(f\"I1: {I1} nats\")\n",
    "print(f\"I2: {I2} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80969aa4",
   "metadata": {},
   "source": [
    "Why is $I2$ (scenario 5) zero? This is because $X$ and $Y$ are independent in this uniform distribution! So, the second property of $I(X;Y)$ holds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e862746",
   "metadata": {},
   "source": [
    "## Mutual information for differential entropy\n",
    "The mutual information $I(X;Y)$ between two random variables with joint density $f(x,y)$ is defined as:\n",
    "\\begin{align}\n",
    " I(X;Y) = \\int_{x\\in X} \\int_{y \\in Y} f(x,y) \\log\\frac{f(x,y)}{f(x)f(y)}\\mathrm{d}x\\mathrm{d}y\\tag{8}\n",
    "\\end{align}\n",
    "\n",
    "Let $(X,Y) \\sim \\mathcal{N}(0,K)$, where\n",
    "\\begin{align}\n",
    " K = \\begin{bmatrix} \\sigma^2 & \\rho \\sigma^2 \\\\ \\rho \\sigma^2 & \\sigma^2 \\end{bmatrix}\\notag.\n",
    "\\end{align}\n",
    "\n",
    "We can show that\n",
    "\\begin{align}\n",
    " I(X;Y) = -\\frac{1}{2}\\log(1-\\sigma^2)\\tag{9}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b4b7c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Real world application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da91305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec42e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1de486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework: lagged mutual information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a98ac1e",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d762dc7",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2047a47",
   "metadata": {},
   "source": [
    "**Ex1**: Given a variable $X_2$ that has four states $a,b,c,$ and $d$, what is its entropy if their probabilities are $P(X=a)=0$, $P(X=b)=0$, $P(X=c)=0$, and $P(X=d)=1$? Please use the natural number as the base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba99988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a3c5734",
   "metadata": {},
   "source": [
    "**Ex2**: In **Ex1**, if their probability follows discretized uniform distribution, what is its entropy? Does it equal to $H_3$ in scenario 3? If not, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee910f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17d9eeeb",
   "metadata": {},
   "source": [
    "**Ex3**: Given two variables $X_3$ and $X_4$, each of which takes on five different realizations/values. Suppose their joint probability follows a discretized uniform distribution, what is the corresponding joint entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be165845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bb3b712",
   "metadata": {},
   "source": [
    "**Ex4**: Please prove the chain rule of the entropy $H(X,Y) = H(X) + H(Y|X)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c405c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0627a6dd",
   "metadata": {},
   "source": [
    "**Ex5**: Please prove Eq.(5) that $h(X) = \\frac{1}{2}\\log(2\\pi e \\sigma^2)$ when $f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{\\frac{-x^2}{2\\sigma^2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee80cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9e393dd",
   "metadata": {},
   "source": [
    "**Ex6**: TBD, related to the data used combined with the binning methods..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c671ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27800d25",
   "metadata": {},
   "source": [
    "## Mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba10e2",
   "metadata": {},
   "source": [
    "**Ex7**: Prove $I(X;Y) = H(Y) - H(Y|X)$ in Eq.(7a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a85ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "171b4d79",
   "metadata": {},
   "source": [
    "**Ex8**: Prove $I(X;Y) = H(X) + H(Y) - H(X,Y)$ in Eq.(7c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed17563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "addfee34",
   "metadata": {},
   "source": [
    "**Ex9**: Prove $I(X;Y) = 0$ when $X$ and $Y$ are independent such that $P(X,Y) = P(X)P(Y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51762c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d14c304",
   "metadata": {},
   "source": [
    "**Ex9**: Please write a function to calculate the conditional entropy $H(X|Y)$ and compute $H(X|Y)$ for the joint probability used in scenario 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7638e44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d15fc92d",
   "metadata": {},
   "source": [
    "**Ex10**: Prove $I(X;Y) = -\\frac{1}{2}\\log(1-\\sigma^2)$ in Eq.(9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebfc1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6b377bf",
   "metadata": {},
   "source": [
    "# Further reading\n",
    "- Shannon, C. E. (1948). A mathematical theory of communication. The Bell system technical journal, 27(3), 379-423.\n",
    "- Cover, T., & Thomas, J. (2006). Chapter 2: Entropy, Relative Entropy, and Mutual Information. Elements of information theory (2nd., Vol. 2). New York: Wiley.\n",
    "- Cover, T., & Thomas, J. (2006). Chapter 8: Differential Entropy. Elements of information theory (2nd., Vol. 2). New York: Wiley.\n",
    "- Gong, W., Yang, D., Gupta, H. V., & Nearing, G. (2014). Estimating information entropy for hydrological data: One‐dimensional case. Water Resources Research, 50(6), 5003-5018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
