{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820fb7e0",
   "metadata": {},
   "source": [
    "This notebook aims to provide a gentle introduction of Shannon's entropy and mutual information.\n",
    "\n",
    "**Author: Peishi Jiang**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f50c89",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# What is information theory?\n",
    "Information theory is a mathematical method to quantify the nonlinear dependencies in a multivariate system. It originates from electronic engineering to study the storage, quantification, and communication of information. Claude Shannon pioneered this field and proposed the concept of Shannon's entropy in the 1940s. Since then, this mathematical branch has gone through great development and applications in various field, including but not limited to, electronic engineering, mathematics, computer science, phyics, neurobiology, and earth science/hydrology.\n",
    "\n",
    "**Reference**: \n",
    "- Shannon, C. E. (1948). A mathematical theory of communication. The Bell system technical journal, 27(3), 379-423.\n",
    "- Cover, T., & Thomas, J. (2006). Elements of information theory (2nd., Vol. 2). New York: Wiley."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ab397",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "We use Shannon's entropy here, denoted of $H$. Given a variable $X$, $H$ quantifies the overall uncertainty (or variability) of $X$ as:\n",
    "\\begin{align}\n",
    " H(X) &= - \\sum_{x \\in X} p(x) \\log(p(x)) \\tag{1},\n",
    "\\end{align}\n",
    "where $p(x)$ is the discretized probability of the realization of $X=x$.\n",
    "\n",
    "For two variables $X$ and $Y$, their overall uncertainty can be calculated through the joint entropy as below:\n",
    "\\begin{align}\n",
    " H(X,Y) &= - \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log(p(x,y)) \\tag{2}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6f075e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Entropy's properties\n",
    "Shannon's entropy has the following nice properties (**Prove them!**):\n",
    "1. Nonnegativity: $H \\geq 0$. In other words, the entropy is always nonnegative.\n",
    "2. Chain rule: $H(X,Y) = H(X) + H(Y|X)$, where $H(Y|X)=-\\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log(p(y|x))$ is the conditional entropy. This property states that the joint uncertainty of $X$ and $Y$ is the sum of $X$'s uncertainty ($H(X)$) and the remaining uncertainty of $Y$ given the knowledge of $X$ ($H(Y|X)$).\n",
    "3. $H(X|Y) \\leq H(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b2f2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Maximum and minimum entropy (univariate case)\n",
    "Now, let's move onto the calculation of the entropy $H$ and see under what conditions it reaches maximum/minimum.\n",
    "\n",
    "OK, say a variable $X$ has two states $X=a$ and $X=b$, with probabilities $P(X=a)$ and $P(X=b)$, respectively, such that $P(X=a) + P(X=b) = 1$. Instead of mannually calculating it (which you can!), we are going to use Python programming to compute it.\n",
    "\n",
    "Let's first define a function to calculate the entropy --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "599bd82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def computeEntropy(pdf, base=np.e):\n",
    "    \"\"\"\n",
    "    Compute the entropy based on pdf\n",
    "    pdf: a numpy array of probability such that sum(pdf)=1\n",
    "    base: the logarithm base\n",
    "    \"\"\"\n",
    "    log_pdf = np.ma.filled(np.log(np.ma.masked_equal(pdf, 0)), 0)\n",
    "    return -np.sum(pdf*log_pdf / np.log(base))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013d7e1",
   "metadata": {},
   "source": [
    "Scenario 1: X is determinant such that $P(X=a)=1$ and $P(X=b)=0$. What is $H(X)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d7bcabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1: -0.0 nats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/q6wyxw8x1_s5rh1mqvfb8vsr0000gn/T/ipykernel_29291/555772398.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  log_pdf = np.ma.filled(np.log(np.ma.masked_equal(pdf, 0)), 0)\n"
     ]
    }
   ],
   "source": [
    "P1 = np.array([1, 0])\n",
    "H1 = computeEntropy(P1)\n",
    "print(f\"H1: {H1} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e0af3",
   "metadata": {},
   "source": [
    "$H(X) = 0$ ! This means there is no variability or uncertainty associated with $X$, which totally makes sense since $X$ is deterministic. Let's consider two additional scenarios:\n",
    "\n",
    "Scenario 2: $P(X=a)=0.1$ and $P(X=b)=0.9$\n",
    "\n",
    "Scenario 3: $P(X=a)=0.5$ and $P(X=b)=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb82d849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2: 0.3250829733914482 nats\n",
      "H3: 0.6931471805599453 nats\n"
     ]
    }
   ],
   "source": [
    "P2, P3 = np.array([0.1, 0.9]), np.array([0.5, 0.5])\n",
    "H2 = computeEntropy(P2)\n",
    "H3 = computeEntropy(P3)\n",
    "print(f\"H2: {H2} nats\")\n",
    "print(f\"H3: {H3} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c720a",
   "metadata": {},
   "source": [
    "Why does $H2 < H3$? \n",
    "\n",
    "Intuitively, this is because scenario 2 is more deterministic than scenario 3 since there is a much higher chance that $X=b$. On the other hand, scenario 3 gives equal probability to $X=a$ and $X=b$.\n",
    "\n",
    "In fact, one can prove that the entropy is maximized when the distribution follows a uniform distribution! Note that scenario 3 is exactly a discretized uniform distribution with only two realizations. In a generalized discrete uniform distribution with $N$ realizations, $H = \\log(\\frac{1}{N})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3fdf8",
   "metadata": {},
   "source": [
    "## What's the units of entropy?\n",
    "It depends on what base is used in the log! If it is a natural base (as what has been used in the previous cases), the units is \"nats\". If the base adopts 2, the units is \"bits\"!\n",
    "\n",
    "Now, let's revisit the three scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0841d6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1: -0.0 bits\n",
      "H2: 0.4689955935892812 bits\n",
      "H3: 1.0 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/q6wyxw8x1_s5rh1mqvfb8vsr0000gn/T/ipykernel_29291/374079235.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  log_pdf = np.ma.filled(np.log(np.ma.masked_equal(pdf, 0)), 0)\n"
     ]
    }
   ],
   "source": [
    "base = 2.\n",
    "H1 = computeEntropy(P1, 2)\n",
    "H2 = computeEntropy(P2, 2)\n",
    "H3 = computeEntropy(P3, 2)\n",
    "print(f\"H1: {H1} bits\")\n",
    "print(f\"H2: {H2} bits\")\n",
    "print(f\"H3: {H3} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d7e34",
   "metadata": {},
   "source": [
    "Now, the results are different. Sometimes, bits are more intuitive for binary cases. Here, $H_3=1$ since $\\log_2(1/2) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5696d0",
   "metadata": {},
   "source": [
    "## Differential entropy\n",
    "\n",
    "We have covered Shannon's entropy applied to the discretized probability space. What about continuous probability distribution? Well, this is where differential entropy is applied (denoted as lower case $h$):\n",
    "\\begin{align}\n",
    " h(X) &= - \\int_{x \\in X} f(x) \\log(f(x)) \\mathrm{d}x\\tag{3},\n",
    "\\end{align}\n",
    "where $f$ is the probability density function of $X$.\n",
    "\n",
    "Differential entropy is very useful when the functional form of $f$ is given and the analytical solutions of $h$ can be derived. Take the uniform distribution that are defined over $[a, b]$ as an example. The corresponding $h$ can be derived as:\n",
    "\\begin{align}\n",
    " h(X) = \\log(b-a)\\tag{4}.\n",
    "\\end{align}\n",
    "\n",
    "For Gaussian distribution $f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{\\frac{-x^2}{2\\sigma^2}}$, the corresponding $h$ is given as:\n",
    "\\begin{align}\n",
    " h(X) = \\frac{1}{2}\\log(2\\pi e \\sigma^2)\\tag{5}.\n",
    "\\end{align}\n",
    "\n",
    "**Note that differential entropy can be negative!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf370eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd111a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb218d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical solution for normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa29509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b4b7c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Flux tower data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da91305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec42e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1de486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework: lagged mutual information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a98ac1e",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d762dc7",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2047a47",
   "metadata": {},
   "source": [
    "**Ex1**: Given a variable $X2$ that has four states $a,b,c,$ and $d$, what is its entropy if their probabilities are $P(X=a)=0$, $P(X=b)=0$, $P(X=c)=0$, and $P(X=d)=1$? Please use the natural number as the base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba99988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a3c5734",
   "metadata": {},
   "source": [
    "**Ex2**: In **Ex1**, if their probability follows discretized uniform distribution, what is its entropy? Does it equal to $H_3$ in scenario 3? If not, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee910f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bb3b712",
   "metadata": {},
   "source": [
    "**Ex3**: Please prove the chain rule of the entropy $H(X,Y) = H(X) + H(Y|X)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c405c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0627a6dd",
   "metadata": {},
   "source": [
    "**Ex4**: Please prove Eq.(5) that $h(X) = \\frac{1}{2}\\log(2\\pi e \\sigma^2)$ when $f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{\\frac{-x^2}{2\\sigma^2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee80cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9e393dd",
   "metadata": {},
   "source": [
    "**Ex5**: TBD, related to the data used combined with the binning methods..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c671ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27800d25",
   "metadata": {},
   "source": [
    "## Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a85ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6b377bf",
   "metadata": {},
   "source": [
    "# Further reading\n",
    "- Shannon, C. E. (1948). A mathematical theory of communication. The Bell system technical journal, 27(3), 379-423.\n",
    "- Cover, T., & Thomas, J. (2006). Chapter 2: Entropy, Relative Entropy, and Mutual Information. Elements of information theory (2nd., Vol. 2). New York: Wiley.\n",
    "- Cover, T., & Thomas, J. (2006). Chapter 8: Differential Entropy. Elements of information theory (2nd., Vol. 2). New York: Wiley.\n",
    "- Gong, W., Yang, D., Gupta, H. V., & Nearing, G. (2014). Estimating information entropy for hydrological data: Oneâ€dimensional case. Water Resources Research, 50(6), 5003-5018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
